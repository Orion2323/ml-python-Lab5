{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89b62465",
   "metadata": {},
   "source": [
    "# CS 5324 Lab 5: Wide and Deep Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f52dbc27",
   "metadata": {},
   "source": [
    "For this assignment, we will be exploring the [Heart Failure Prediction](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction?resource=download) dataset. It is a collection of datasets combined into one large dataset. This dataset is composed of observations regarding patients' health traits related to the likelihood of heart failure.\n",
    "\n",
    "This dataset was sourced from [Kaggle](https://www.kaggle.com/datasets) and consists of 918 observations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3052a4e",
   "metadata": {},
   "source": [
    "## Team"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91760563",
   "metadata": {},
   "source": [
    "The team consists of three members:\n",
    "1. Melodie Zhu\n",
    "2. Samina Faheem\n",
    "3. Giancarlos Dominguez"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c838ac1",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af8dbe65",
   "metadata": {},
   "source": [
    "Let us import our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d9e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraies\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb375a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset from csv file\n",
    "data_directory = os.getcwd() + '\\\\data\\\\heart.csv'\n",
    "df = pd.read_csv(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d20ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecfe47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the dataset\", df.shape)\n",
    "\n",
    "print(f\"\\nNumber of observations in the dataset: {df.shape[0]}\")\n",
    "print(f\"Number of features in the dataset: {df.shape[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1840d1e",
   "metadata": {},
   "source": [
    "Next, we will check for any duplicate observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f7c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "print(f\"\\nNumber of duplicate rows: {df.duplicated().sum()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87fd4037",
   "metadata": {},
   "source": [
    "Luckily, we don't have to worry about duplicate rows. Now, let us check for rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "print(f\"\\nNumber of missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edbe3bce",
   "metadata": {},
   "source": [
    "None of our observations have missing values. Therefore, we don't have to worry about holes in our data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3c6df2d",
   "metadata": {},
   "source": [
    "### Defining Class Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6696e73b",
   "metadata": {},
   "source": [
    "| Variable Name | Datatype | Description | Values |\n",
    ":------: | :------: | :------: | :------:|\n",
    "| `Age` | Numerical int | How old the patient is (yrs) | NA |\n",
    "| `Sex` | Categorical str | The biological gender of the patient | **M**: Male, **F**: Female |\n",
    "| `ChestPainType` | Categorical str | The chest pain condition of the patient | **TA**: Typical Angina, **ATA**: Atypical Angina, **NAP**: Non-Anginal Pain, **ASY**: Asymptomatic |\n",
    "| `RestingBP` | Numerical int | The resting blood pressure (mmHg) | NA |\n",
    "| `Cholesterol` | Numerical int | The cholesterol level of the patient (mm/dl) | NA |\n",
    "| `FastingBS` | Categorical int | The fasting blood sugar level of the patient | **1**: if FastingBS > 120 (mg/dl), **0**: otherwise |\n",
    "| `RestingECG` | Categorical str | The resting electrocardiogram results of the patient | **Normal**: Normal, **ST**: Having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), **LVH**: Showing probable or definite left ventricular hypertrophy by Estes' criteria |\n",
    "| `MaxHR` | Numerical int | The maximum heart rate recorded in the patient | NA |\n",
    "| `ExerciseAngina` | Categorical str | Whether the patient has exersice-induced angina | **Y**: Yes, **N**: No |\n",
    "| `OldPeak` | Numerical float | ST segment value of the patient | NA |\n",
    "| `ST_Slope` | Categorical str | Slope of the peak exercise ST segment | **Up**: Upsloping, **Flat**: Flat, **Down**: Downsloping |\n",
    "| `HeartDisease` | Categorical int | Whether the patient is likely to have heart failure| **1**: Likely to have heart failure, **0**: Not likely to have heart failure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b533efa4",
   "metadata": {},
   "source": [
    "Since all of our features are medically related to heart conditions and failure, we will not remove any features from our dataset.<br>\n",
    "\n",
    "Now, we will convert our categorical variables into integers and standardize our numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ebe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_imputed = copy.deepcopy(df)\n",
    "\n",
    "# define vars to encode as integer    \n",
    "encoders = dict()\n",
    "categorical_headers = [\n",
    "    'Sex',\n",
    "    'ChestPainType',\n",
    "    'RestingECG',\n",
    "    'ExerciseAngina',\n",
    "    'ST_Slope'\n",
    "]\n",
    "\n",
    "# train all encoders\n",
    "for col in categorical_headers:\n",
    "    df_imputed[col] = df_imputed[col].str.strip()\n",
    "    \n",
    "    if col == 'HeartDisease':\n",
    "        tmp = LabelEncoder()\n",
    "        df_imputed[col] = tmp.fit_transform(df_imputed[col])\n",
    "    else:\n",
    "        # integer encode strings that are features\n",
    "        encoders[col] = LabelEncoder() # save the encoder\n",
    "        df_imputed[col+'_int'] = encoders[col].fit_transform(df_imputed[col])\n",
    "\n",
    "# scale numeric, continuous variables\n",
    "numeric_headers = [\n",
    "    \"Age\", \n",
    "    \"RestingBP\", \n",
    "    \"Cholesterol\",\n",
    "    \"MaxHR\"\n",
    "]\n",
    "\n",
    "ss = StandardScaler()\n",
    "df_imputed[numeric_headers] = ss.fit_transform(df_imputed[numeric_headers])\n",
    "\n",
    "include_header =[\"FastingBS\",\"Oldpeak\"]\n",
    "df_imputed.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4eeacfe9",
   "metadata": {},
   "source": [
    "It should be noted that we encoded our categorical variables into integers. The column names will be denoted by their original name with \"_int\" added at the end, but their context to the dataset will not change. Below are the variables we will use throughout the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_headers_ints = [x+'_int' for x in categorical_headers]\n",
    "feature_columns = categorical_headers_ints+numeric_headers+include_header\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "print(f\"We will use the following {len(feature_columns)} features:\")\n",
    "pp.pprint(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa892117",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCategorical String Headers:\")\n",
    "pp.pprint(categorical_headers) # string data\n",
    "\n",
    "print(\"\\nCategorical Headers, Encoded as Integer:\")\n",
    "pp.pprint(categorical_headers_ints) # string data encoded as an integer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b52db25",
   "metadata": {},
   "source": [
    "### Cross Product Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46cd667a",
   "metadata": {},
   "source": [
    "Our dataset has 12 features, but we will focus on the categorical features (excluding our target feature) to make cross-product features. We will make a prediction on cross-product features we believe are best for our models to test.\n",
    "\n",
    "`ChestPainType` & `ExerciseAngina`: The `ChestPainType` features considers differents types of angina. By merging the `ChestPainType` and `ExerciseAngina`, we can explore if there is any correlation between the different angina types and whether they were caused by exercise.\n",
    "\n",
    "`FastingBS` & `RestingECG`: The changes in a patient's fasting blood sugar level can influence the heart's electrical activity. By combining these features, we hope to highlight any correlation between a patient's fasting blood sugar level and their electrocardiogram results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bf66f90",
   "metadata": {},
   "source": [
    "### Evaluation Method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aa8cca2",
   "metadata": {},
   "source": [
    "For evaluating our models' performance, we will not be using accuracy, since it is prone to being skewed depending on how balanced the dataset is, among other factors. Instead, we will be using the recall evaluation method. We will use this method due to the medical nature of our dataset, making it necessary to account for true positives and false negatives. Predicting false negative have higher consequences in a medical dataset. Hence, Recall should be used instead of accuracy due to the importance of false negatives for this dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf437a4d",
   "metadata": {},
   "source": [
    "### Dataset Splitting Method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b446802e",
   "metadata": {},
   "source": [
    "For this dataset, we will split it using the stratified 10-fold cross validation. Since we only have 918 observations, we need to maximize the use of our dataset. This method will also ensure that the subsets are as evenly distributed like the class ratio of the overall dataset. By using stratified 10-folds, we ensure that the sub-datasets will not be more unbalanced. Applying this method with the target variable ensures that the cross-validation result is a close approximation of generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a53464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, cross_val_score\n",
    "\n",
    "# predict X and y data:\n",
    "X = df_imputed[feature_columns].to_numpy()\n",
    "y = df_imputed['HeartDisease'].values\n",
    "\n",
    "# get size of dataset, number of features, and number of classes\n",
    "n_samples, n_features = X.shape\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "print(\"Number of observations in dataset: {}\".format(n_samples))\n",
    "print(\"Number of features in dataset: {}\".format(n_features))\n",
    "print(\"Number of classes for the target feature (HeartDisease): {}\".format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b074fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "print('Number of instances in each class:'+str(np.bincount(y)))\n",
    "\n",
    "# plot piechart of the number of instances in each class\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.pie(np.bincount(y), labels=['No Heart Disease', 'Heart Disease'], autopct='%1.2f%%')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46acd214",
   "metadata": {},
   "source": [
    "From the pie chart above, we observe that the dataset classes are unbalanced. There are more patients with the likelihood of having heart failure than those not likely to have heart disease. With stratified 10-fold cross validation, all of our subsets will have the same 55/44 class ratio to prevent it from becoming further unbalanced.\n",
    "\n",
    "Now, we will split our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d13e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "cv_object = StratifiedKFold(n_splits=num_cv_iterations)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dcb0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "iter_num=0\n",
    "\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc371e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# select model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X,y)\n",
    "\n",
    "#select cross validation\n",
    "cv = StratifiedShuffleSplit(n_splits=10)\n",
    "# select evaluation criteria\n",
    "my_scorer = make_scorer(recall_score)\n",
    "\n",
    "# run model training and cross validation\n",
    "per_fold_eval_criteria = cross_val_score(estimator=clf,\n",
    "                                    X=X,\n",
    "                                    y=y,\n",
    "                                    cv=cv,\n",
    "                                    scoring=my_scorer\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.bar(range(len(per_fold_eval_criteria)),per_fold_eval_criteria)\n",
    "plt.ylim([min(per_fold_eval_criteria)-0.01,max(per_fold_eval_criteria)])\n",
    "\n",
    "# title and labels\n",
    "plt.title('Recall Score per Fold')\n",
    "plt.ylabel('Recall Score')\n",
    "plt.xlabel('Fold Number')\n",
    "\n",
    "print(f'Recall score average: {per_fold_eval_criteria.mean()*100}')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a70afea1",
   "metadata": {},
   "source": [
    "Some of our folds have a recall value of 0.9. The closer to 1 the recall score is, the better our model's performance. Since our average recall value among the folds is in the high 0.8's, we can say that our model is performing well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b147127e",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d2ff73f",
   "metadata": {},
   "source": [
    "We will create five different wide and deep networks. The first three will consist of different cross-product features combinations.\n",
    "\n",
    "The 4th and fifth model with have a different number of layers.\n",
    "\n",
    "Lastly, we will compare our best performing model from the five models to a deep network (without the wide branch.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c05bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "cat_X = df_imputed[categorical_headers_ints].to_numpy()\n",
    "cat_y = df_imputed['HeartDisease'].values\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(cat_X,cat_y): \n",
    "    X_train_cat = cat_X[train_indices]\n",
    "    y_train_cat = cat_y[train_indices]\n",
    "    \n",
    "    X_test_cat = cat_X[test_indices]\n",
    "    y_test_cat = cat_y[test_indices]\n",
    "    \n",
    "num_X = df_imputed[categorical_headers_ints].to_numpy()\n",
    "num_y = df_imputed['HeartDisease'].values # get the labels we want\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(num_X,num_y): \n",
    "    X_train_num = num_X[train_indices]\n",
    "    y_train_num = num_y[train_indices]\n",
    "    \n",
    "    X_test_num = num_X[test_indices]\n",
    "    y_test_num = num_y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_headers:\n",
    "    vals = df_imputed[col].unique()\n",
    "    print(col,'has', len(vals), 'unique values:')\n",
    "    print(vals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef0438c5",
   "metadata": {},
   "source": [
    "### First Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fed31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_columns = [['ST_Slope','RestingECG']]\n",
    "\n",
    "# cross each set of columns in the list above\n",
    "cross_col_df_names = []\n",
    "for cols_list in cross_columns:\n",
    "    # encode as ints for the embedding\n",
    "    enc = LabelEncoder()\n",
    "    \n",
    "    # 1. create crossed labels by join operation\n",
    "    X_crossed_train = df_imputed[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "    \n",
    "    # get a nice name for this new crossed column\n",
    "    cross_col_name = '_'.join(cols_list)\n",
    "    \n",
    "    # 2. encode as integers\n",
    "    enc.fit(np.hstack((X_crossed_train.to_numpy())))\n",
    "    \n",
    "    # 3. Save into dataframe with new name\n",
    "    df_imputed[cross_col_name] = enc.transform(X_crossed_train)\n",
    "    \n",
    "    # keep track of the new names of the crossed columns\n",
    "    cross_col_df_names.append(cross_col_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ec829",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_X = df_imputed[cross_col_df_names].to_numpy()\n",
    "cross_y = df_imputed['HeartDisease'].values\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(cross_X,cross_y): \n",
    "    X_train_crossed = cross_X[train_indices]\n",
    "    y_train_crossed = cross_y[train_indices]\n",
    "    \n",
    "    X_test_crossed = cross_X[test_indices]\n",
    "    y_test_crossed = cross_y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a611b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossed_outputs = []\n",
    "\n",
    "# CROSSED DATA INPUT\n",
    "input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='wide_inputs')\n",
    "for idx,col in enumerate(cross_col_df_names):\n",
    "    N = df_imputed[col].max()+1\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_crossed, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "    \n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = concatenate(crossed_outputs, name='wide_concat')\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# CATEGORICAL DATA INPUT\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical_input')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "    N = df_imputed[col].max()+1\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "    \n",
    "# create dense input branch for numeric\n",
    "input_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=22, activation='relu',name='num_1')(input_num)\n",
    "    \n",
    "all_deep_branch_outputs.append(x_dense)\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = concatenate(all_deep_branch_outputs,name='concat_embeds')\n",
    "deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "deep_branch = Dense(units=25,activation='relu', name='deep2')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "    \n",
    "# merge the deep and wide branch\n",
    "final_branch = concatenate([wide_branch, deep_branch],\n",
    "                           name='concat_deep_wide')\n",
    "final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "model = Model(inputs=[input_crossed,input_cat,input_num], \n",
    "              outputs=final_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1655a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model \n",
    "\n",
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd0fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Recall\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[Recall(),AUC(from_logits=True)])\n",
    "\n",
    "history1 = model.fit([X_train_crossed,X_train_cat,X_train_num],\n",
    "                    y_train, \n",
    "                    epochs=15, \n",
    "                    batch_size=32, \n",
    "                    verbose=1, \n",
    "                    validation_data = ([X_test_crossed,X_test_cat,X_test_num],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9fe5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "yhat1 = np.round(model.predict([X_test_crossed,X_test_cat,X_test_num]))\n",
    "\n",
    "# create confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test, yhat1)\n",
    "\n",
    "# create number of values in each group\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "\n",
    "# create percentages of each group\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                    cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "\n",
    "# create labels\n",
    "group_names = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a22df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mt.classification_report(y_test,yhat1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a40cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist1Keys = list(history1.history.keys())\n",
    "print(hist1Keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885b608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(history1.history[hist1Keys[1]])\n",
    "\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('epochs')\n",
    "plt.title('Training')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(history1.history[hist1Keys[4]])\n",
    "plt.title('Validation')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(history1.history[hist1Keys[0]])\n",
    "plt.ylabel('Training Loss')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(history1.history[hist1Keys[3]])\n",
    "plt.xlabel('epochs')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28df765d",
   "metadata": {},
   "source": [
    "### Second Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01453a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_columns = [\n",
    "    ['ChestPainType','RestingECG'],\n",
    "    ['Sex','ExerciseAngina','ST_Slope']\n",
    "]\n",
    "\n",
    "# cross each set of columns in the list above\n",
    "cross_col_df_names = []\n",
    "for cols_list in cross_columns:\n",
    "    # encode as ints for the embedding\n",
    "    enc = LabelEncoder()\n",
    "    \n",
    "    # 1. create crossed labels by join operation\n",
    "    X_crossed_train = df_imputed[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "    \n",
    "    # get a nice name for this new crossed column\n",
    "    cross_col_name = '_'.join(cols_list)\n",
    "    \n",
    "    # 2. encode as integers\n",
    "    enc.fit(np.hstack((X_crossed_train.to_numpy())))\n",
    "    \n",
    "    # 3. Save into dataframe with new name\n",
    "    df_imputed[cross_col_name] = enc.transform(X_crossed_train)\n",
    "    \n",
    "    # keep track of the new names of the crossed columns\n",
    "    cross_col_df_names.append(cross_col_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_X = df_imputed[cross_col_df_names].to_numpy()\n",
    "cross_y = df_imputed['HeartDisease'].values\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(cross_X,cross_y): \n",
    "    X_train_crossed = cross_X[train_indices]\n",
    "    y_train_crossed = cross_y[train_indices]\n",
    "    \n",
    "    X_test_crossed = cross_X[test_indices]\n",
    "    y_test_crossed = cross_y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossed_outputs = []\n",
    "\n",
    "# CROSSED DATA INPUT\n",
    "input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='wide_inputs')\n",
    "for idx,col in enumerate(cross_col_df_names):\n",
    "    N = df_imputed[col].max()+1\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_crossed, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = concatenate(crossed_outputs, name='wide_concat')\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# CATEGORICAL DATA INPUT\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical_input')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "    N = df_imputed[col].max()+1\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "\n",
    "# create dense input branch for numeric\n",
    "input_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=22, activation='relu',name='num_1')(input_num)\n",
    "    \n",
    "all_deep_branch_outputs.append(x_dense)\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = concatenate(all_deep_branch_outputs,name='concat_embeds')\n",
    "deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "deep_branch = Dense(units=25,activation='relu', name='deep2')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "    \n",
    "# merge the deep and wide branch\n",
    "final_branch = concatenate([wide_branch, deep_branch],\n",
    "                           name='concat_deep_wide')\n",
    "final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "model = Model(inputs=[input_crossed,input_cat,input_num], \n",
    "              outputs=final_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d5000",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3645d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[Recall(),AUC(from_logits=True)])\n",
    "\n",
    "history2 = model.fit([X_train_crossed,X_train_cat,X_train_num],\n",
    "                    y_train, \n",
    "                    epochs=15, \n",
    "                    batch_size=32, \n",
    "                    verbose=1, \n",
    "                    validation_data = ([X_test_crossed,X_test_cat,X_test_num],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat2 = np.round(model.predict([X_test_crossed,X_test_cat,X_test_num]))\n",
    "\n",
    "# create confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test, yhat2)\n",
    "\n",
    "# create number of values in each group\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "\n",
    "# create percentages of each group\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                      cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "\n",
    "# create labels\n",
    "group_names = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mt.classification_report(y_test,yhat2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa1ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2Keys = list(history2.history.keys())\n",
    "print(hist2Keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f13a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(history2.history[hist2Keys[1]])\n",
    "\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Training')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(history2.history[hist2Keys[4]])\n",
    "plt.title('Validation')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(history2.history[hist2Keys[0]])\n",
    "plt.ylabel('Training Loss')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(history2.history[hist2Keys[3]])\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41a61f78",
   "metadata": {},
   "source": [
    "### Third Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a7f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_columns = [\n",
    "    ['ChestPainType','RestingECG','ST_Slope'],\n",
    "    ['Sex','ExerciseAngina'],\n",
    "    ['ChestPainType','Sex']\n",
    "]\n",
    "\n",
    "# cross each set of columns in the list above\n",
    "cross_col_df_names = []\n",
    "for cols_list in cross_columns:\n",
    "    # encode as ints for the embedding\n",
    "    enc = LabelEncoder()\n",
    "    \n",
    "    # 1. create crossed labels by join operation\n",
    "    X_crossed_train = df_imputed[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "    \n",
    "    # get a nice name for this new crossed column\n",
    "    cross_col_name = '_'.join(cols_list)\n",
    "    \n",
    "    # 2. encode as integers\n",
    "    enc.fit(np.hstack((X_crossed_train.to_numpy())))\n",
    "    \n",
    "    # 3. Save into dataframe with new name\n",
    "    df_imputed[cross_col_name] = enc.transform(X_crossed_train)\n",
    "    \n",
    "    # keep track of the new names of the crossed columns\n",
    "    cross_col_df_names.append(cross_col_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16718dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_X = df_imputed[cross_col_df_names].to_numpy()\n",
    "cross_y = df_imputed['HeartDisease'].values\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(cross_X,cross_y): \n",
    "    X_train_crossed = cross_X[train_indices]\n",
    "    y_train_crossed = cross_y[train_indices]\n",
    "    \n",
    "    X_test_crossed = cross_X[test_indices]\n",
    "    y_test_crossed = cross_y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069fecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossed_outputs = []\n",
    "\n",
    "# CROSSED DATA INPUT\n",
    "input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='wide_inputs')\n",
    "for idx,col in enumerate(cross_col_df_names):\n",
    "    N = df_imputed[col].max()+1\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_crossed, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "    \n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = concatenate(crossed_outputs, name='wide_concat')\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# CATEGORICAL DATA INPUT\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical_input')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "    N = df_imputed[col].max()+1\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "    \n",
    "# create dense input branch for numeric\n",
    "input_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=22, activation='relu',name='num_1')(input_num)\n",
    "    \n",
    "all_deep_branch_outputs.append(x_dense)\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = concatenate(all_deep_branch_outputs,name='concat_embeds')\n",
    "deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "deep_branch = Dense(units=25,activation='relu', name='deep2')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "    \n",
    "# merge the deep and wide branch\n",
    "final_branch = concatenate([wide_branch, deep_branch],\n",
    "                           name='concat_deep_wide')\n",
    "final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "model = Model(inputs=[input_crossed,input_cat,input_num], \n",
    "              outputs=final_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f7a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fd24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[Recall(),AUC(from_logits=True)])\n",
    "\n",
    "history3 = model.fit([X_train_crossed,X_train_cat,X_train_num],\n",
    "                    y_train, \n",
    "                    epochs=15, \n",
    "                    batch_size=32, \n",
    "                    verbose=1, \n",
    "                    validation_data = ([X_test_crossed,X_test_cat,X_test_num],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8814e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat3 = np.round(model.predict([X_test_crossed,X_test_cat,X_test_num]))\n",
    "\n",
    "# create confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test, yhat3)\n",
    "\n",
    "# create number of values in each group\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "\n",
    "# create percentages of each group\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                      cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "\n",
    "# create labels\n",
    "group_names = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mt.classification_report(y_test,yhat3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7707a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist3Keys = list(history3.history.keys())\n",
    "print(hist3Keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(history3.history[hist3Keys[1]])\n",
    "\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Training')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(history3.history[hist3Keys[4]])\n",
    "plt.title('Validation')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(history3.history[hist3Keys[0]])\n",
    "plt.ylabel('Training Loss')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(history3.history[hist3Keys[3]])\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfe374ee",
   "metadata": {},
   "source": [
    "### Different Number of Layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "470a3510",
   "metadata": {},
   "source": [
    "Next, we will create two wide and deep networks that have a different number of layers and compare their performance to each other."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8dba73a8",
   "metadata": {},
   "source": [
    "### One Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d364cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossed_outputs = []\n",
    "\n",
    "# CROSSED DATA INPUT\n",
    "input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='wide_inputs')\n",
    "for idx,col in enumerate(cross_col_df_names):\n",
    "    N = df_imputed[col].max()+1\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_crossed, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = concatenate(crossed_outputs, name='wide_concat')\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# CATEGORICAL DATA INPUT\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical_input')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "    N = df_imputed[col].max()+1\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "    \n",
    "# create dense input branch for numeric\n",
    "input_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=22, activation='relu',name='num_1')(input_num)\n",
    "    \n",
    "all_deep_branch_outputs.append(x_dense)\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = concatenate(all_deep_branch_outputs,name='concat_embeds')\n",
    "deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "    \n",
    "# merge the deep and wide branch\n",
    "final_branch = concatenate([wide_branch, deep_branch],\n",
    "                           name='concat_deep_wide')\n",
    "final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "model = Model(inputs=[input_crossed,input_cat,input_num], \n",
    "              outputs=final_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fc419",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[Recall(),AUC(from_logits=True)])\n",
    "\n",
    "history4 = model.fit([X_train_crossed,X_train_cat,X_train_num],\n",
    "                    y_train, \n",
    "                    epochs=15, \n",
    "                    batch_size=32, \n",
    "                    verbose=1, \n",
    "                    validation_data = ([X_test_crossed,X_test_cat,X_test_num],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e77d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat4 = np.round(model.predict([X_test_crossed,X_test_cat,X_test_num]))\n",
    "\n",
    "# create confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test, yhat4)\n",
    "\n",
    "# create number of values in each group\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "\n",
    "# create percentages of each group\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                      cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "\n",
    "# create labels\n",
    "group_names = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62048a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mt.classification_report(y_test,yhat4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac61ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist4Keys = list(history4.history.keys())\n",
    "print(hist4Keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b3ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(history4.history[hist4Keys[1]])\n",
    "\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Training')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(history4.history[hist4Keys[4]])\n",
    "plt.title('Validation')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(history4.history[hist4Keys[0]])\n",
    "plt.ylabel('Training Loss')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(history4.history[hist4Keys[3]])\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a4e51dd",
   "metadata": {},
   "source": [
    "### Four Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c125b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossed_outputs = []\n",
    "\n",
    "# CROSSED DATA INPUT\n",
    "input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='wide_inputs')\n",
    "for idx,col in enumerate(cross_col_df_names):\n",
    "    N = df_imputed[col].max()+1\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_crossed, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "    \n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = concatenate(crossed_outputs, name='wide_concat')\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# CATEGORICAL DATA INPUT\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical_input')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "    N = df_imputed[col].max()+1\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "    \n",
    "# create dense input branch for numeric\n",
    "input_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=22, activation='relu',name='num_1')(input_num)\n",
    "    \n",
    "all_deep_branch_outputs.append(x_dense)\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = concatenate(all_deep_branch_outputs,name='concat_embeds')\n",
    "deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "deep_branch = Dense(units=25,activation='relu', name='deep2')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "deep_branch = Dense(units=5,activation='relu', name='deep4')(deep_branch)\n",
    "\n",
    "# merge the deep and wide branch\n",
    "final_branch = concatenate([wide_branch, deep_branch],\n",
    "                           name='concat_deep_wide')\n",
    "final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "model = Model(inputs=[input_crossed,input_cat,input_num], \n",
    "              outputs=final_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5956f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9956988",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[Recall(),AUC(from_logits=True)])\n",
    "\n",
    "history5 = model.fit([X_train_crossed,X_train_cat,X_train_num],\n",
    "                    y_train, \n",
    "                    epochs=15, \n",
    "                    batch_size=32, \n",
    "                    verbose=1, \n",
    "                    validation_data = ([X_test_crossed,X_test_cat,X_test_num],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ccd08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat5 = np.round(model.predict([X_test_crossed,X_test_cat,X_test_num]))\n",
    "\n",
    "# create confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test, yhat5)\n",
    "\n",
    "# create number of values in each group\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "\n",
    "# create percentages of each group\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                      cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "\n",
    "# create labels\n",
    "group_names = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f6753",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mt.classification_report(y_test,yhat5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist5Keys = list(history5.history.keys())\n",
    "print(hist5Keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dfc98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(history5.history[hist5Keys[1]])\n",
    "\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Training')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(history5.history[hist5Keys[4]])\n",
    "plt.title('Validation')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(history5.history[hist5Keys[0]])\n",
    "plt.ylabel('Training Loss')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(history5.history[hist5Keys[3]])\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e6a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mt.classification_report(y_test,yhat5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e02f8721",
   "metadata": {},
   "source": [
    "### Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9bea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print recall results of each model\n",
    "print(\"Recall of first combination\",mt.recall_score(y_test,yhat1))\n",
    "print(\"Recall of second combination\",mt.recall_score(y_test,yhat2))\n",
    "print(\"Recall of third combination\",mt.recall_score(y_test,yhat3))\n",
    "print(\"Recall of one layer deep\",mt.recall_score(y_test,yhat4))\n",
    "print(\"Recall of four layers deep\",mt.recall_score(y_test,yhat5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5220de2",
   "metadata": {},
   "source": [
    "Out of all our models, `model x` performed the best. `blah blah blah`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38ce7076",
   "metadata": {},
   "source": [
    "### Deep Network vs Wide and Deep Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb7d85b5",
   "metadata": {},
   "source": [
    "Next, we will create a deep network (without a wide branch) and compare its results to a wide and deep network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95395c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to create separate lists for each branch\n",
    "crossed_outputs = []\n",
    "\n",
    "# CROSSED DATA INPUT\n",
    "input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='deep_inputs')\n",
    "for idx,col in enumerate(cross_col_df_names):\n",
    "    N = df_imputed[col].max()+1\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_crossed, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "\n",
    "# NO WIDE BRANCH\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "#wide_branch = concatenate(crossed_outputs, name='wide_concat')\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# CATEGORICAL DATA INPUT\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical_input')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "    N = df_imputed[col].max()+1\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "\n",
    "# create dense input branch for numeric\n",
    "input_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=22, activation='relu',name='num_1')(input_num)\n",
    "    \n",
    "all_deep_branch_outputs.append(x_dense)\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = concatenate(all_deep_branch_outputs,name='concat_embeds')\n",
    "deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "deep_branch = Dense(units=25,activation='relu', name='deep2')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "deep_branch = Dense(units=5,activation='relu', name='deep4')(deep_branch)    \n",
    "\n",
    "# merge the deep and wide branch\n",
    "final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(deep_branch)\n",
    "\n",
    "model = Model(inputs=[input_crossed,input_cat,input_num], \n",
    "              outputs=final_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b57b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc818449",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[Recall(),AUC(from_logits=True)])\n",
    "\n",
    "history6 = model.fit([X_train_crossed,X_train_cat,X_train_num],\n",
    "                    y_train, \n",
    "                    epochs=15, \n",
    "                    batch_size=32, \n",
    "                    verbose=1, \n",
    "                    validation_data = ([X_test_crossed,X_test_cat,X_test_num],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d789d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat6 = np.round(model.predict([X_test_crossed,X_test_cat,X_test_num]))\n",
    "\n",
    "# create confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test, yhat6)\n",
    "\n",
    "# create number of values in each group\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "\n",
    "# create percentages of each group\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                      cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "\n",
    "# create labels\n",
    "group_names = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b1a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mt.classification_report(y_test,yhat6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f68038",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist6Keys = list(history6.history.keys())\n",
    "print(hist6Keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fe9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(history6.history[hist6Keys[1]])\n",
    "\n",
    "plt.ylabel('Accuracy %')\n",
    "plt.title('Training')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(history6.history[hist6Keys[3]])\n",
    "plt.title('Validation')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(history6.history[hist6Keys[0]])\n",
    "plt.ylabel('Training Loss')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(history6.history[hist6Keys[2]])\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adb69a42",
   "metadata": {},
   "source": [
    "### Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8425ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Four layers deep model recall value: \",mt.recall_score(y_test,yhat4))\n",
    "\n",
    "print(\"Deep network recall value: \",mt.recall_score(y_test,yhat6))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6c5f96d",
   "metadata": {},
   "source": [
    "Our four layer model outperforms the deep network, most likely due to the four layer model having both the wide and deep branch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5bb8346",
   "metadata": {},
   "source": [
    "## Graduate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eba50b26",
   "metadata": {},
   "source": [
    "### Embedded Weights and Dimension Reduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71ecd21f",
   "metadata": {},
   "source": [
    "We captured the ourput of the embedded layers across our models. We will perform dimension reduction using PCA and plot its results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e840717",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights() # list of numpy arrays\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X) # fit data and then transform it\n",
    "X_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6468aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.set(style=\"darkgrid\") \n",
    "def get_feature_names_from_weights(weights, names):\n",
    "    tmp_array = []\n",
    "    for comp in weights:\n",
    "        tmp_string = ''\n",
    "        for fidx,f in enumerate(names):\n",
    "            if fidx>0 and comp[fidx]>=0:\n",
    "                tmp_string+='+'\n",
    "            tmp_string += '%.2f*%s ' % (comp[fidx],f[:-5])\n",
    "        tmp_array.append(tmp_string)\n",
    "    return tmp_array\n",
    "  \n",
    "plt.style.use('default')\n",
    "pca_weight_strings = get_feature_names_from_weights(pca.components_, feature_columns) \n",
    "\n",
    "# create some pandas dataframes from the transformed outputs\n",
    "df_pca = pd.DataFrame(X_pca,columns=[pca_weight_strings])\n",
    "\n",
    "from matplotlib.pyplot import scatter\n",
    "\n",
    "# scatter plot the output, with the names created from the weights\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.xlabel(pca_weight_strings[0]) \n",
    "plt.ylabel(pca_weight_strings[1])\n",
    "\n",
    "plt.title(\"Dimension Reduction on Embedded Layers Output\")\n",
    "\n",
    "ax = scatter(X_pca[:,0], X_pca[:,1], c=y, s=(y+2)*10, cmap=cmap)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb33c270",
   "metadata": {},
   "source": [
    "The above scatterplot shows two different colors (for the two classes in our target feature). The intersection of these plot points highlights how some of the features in this dataset can greatly influence whether a patient is at risk of heart failure (*class 1*) or not (*class 0*). Due to the close intersection of most plot points, we can say that most of the features in this dataset are important when considering how well any model can predict heart failure using this dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "085ee925",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "772002e4",
   "metadata": {},
   "source": [
    "An ROC (Receiver Operating Charcateristic) Curve is a type of graph that shows the performance of a model at all classification threshholds between the True Positive rate (recall) and False Positive Rate.\n",
    "\n",
    "The AUC (Area Under the Curve) provides an aggregate visual of the performance across the classification thresholds. The AUC is measured from a range of 0-1, and the more accurate our model is, the closer to 1.0 its AUC value is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d24bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_sk = LogisticRegression(max_iter = 100) # all params default\n",
    "\n",
    "lr_sk.fit(X_train, y_train) \n",
    "\n",
    "yhat8 = lr_sk.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16461c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import label_binarize\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "X =  df_imputed[numeric_headers].values\n",
    "y = df_imputed['HeartDisease'].values\n",
    "\n",
    "# Binarize the output\n",
    "y = label_binarize(y, classes=[0, 1])\n",
    "\n",
    "n_classes = df_imputed['HeartDisease'].unique()\n",
    "\n",
    "# Add noisy features to make the problem harder\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "X = np.c_[X, random_state.randn(n_samples, 200* n_features)]\n",
    "\n",
    "# shuffle and split training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2,random_state=0)\n",
    "\n",
    "# Learn to predict each class against the other\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
    "                                 random_state=random_state))\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d5092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, roc_auc = roc_curve(y_test.flatten(), y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bcd119",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % auc(fpr,tpr))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "\n",
    "plt.title('Receiver Operating Characteristic Curve of Dataset')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79090d15",
   "metadata": {},
   "source": [
    "Our model has an AUC value of 0.61, meaning the model is correctly predicting more than half its test cases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94813953",
   "metadata": {},
   "source": [
    "Code used in this notebook was adapted from Professor Larson's notebooks for this class.<br>\n",
    "We appreciate Professor Larson's models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

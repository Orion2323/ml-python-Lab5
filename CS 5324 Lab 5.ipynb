{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89b62465",
   "metadata": {},
   "source": [
    "# CS 5324 Lab 5: Wide and Deep Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f52dbc27",
   "metadata": {},
   "source": [
    "For this assignment, we will be exploring the [Heart Failure Prediction](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction?resource=download) dataset. It is a collection of datasets combined into one large dataset. This dataset is composed of observations regarding patients' health traits related to the likelihood of heart failure.\n",
    "\n",
    "This dataset was sourced from [Kaggle](https://www.kaggle.com/datasets) and consists of 918 observations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3052a4e",
   "metadata": {},
   "source": [
    "## Team"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91760563",
   "metadata": {},
   "source": [
    "The team consists of three members:\n",
    "1. Melodie Zhu\n",
    "2. Samina Faheem\n",
    "3. Giancarlos Dominguez"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c838ac1",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af8dbe65",
   "metadata": {},
   "source": [
    "Let us import our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6d9e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraies\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb375a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset from csv file\n",
    "data_directory = os.getcwd() + '\\\\data\\\\heart.csv'\n",
    "df = pd.read_csv(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "613d20ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>F</td>\n",
       "      <td>NAP</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>156</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>130</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>ST</td>\n",
       "      <td>98</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>ASY</td>\n",
       "      <td>138</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>108</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>M</td>\n",
       "      <td>NAP</td>\n",
       "      <td>150</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>122</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n",
       "0   40   M           ATA        140          289          0     Normal    172   \n",
       "1   49   F           NAP        160          180          0     Normal    156   \n",
       "2   37   M           ATA        130          283          0         ST     98   \n",
       "3   48   F           ASY        138          214          0     Normal    108   \n",
       "4   54   M           NAP        150          195          0     Normal    122   \n",
       "\n",
       "  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
       "0              N      0.0       Up             0  \n",
       "1              N      1.0     Flat             1  \n",
       "2              N      0.0       Up             0  \n",
       "3              Y      1.5     Flat             1  \n",
       "4              N      0.0       Up             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ecfe47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset (918, 12)\n",
      "\n",
      "Number of observations in the dataset: 918\n",
      "Number of features in the dataset: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the dataset\", df.shape)\n",
    "\n",
    "print(f\"\\nNumber of observations in the dataset: {df.shape[0]}\")\n",
    "print(f\"Number of features in the dataset: {df.shape[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1840d1e",
   "metadata": {},
   "source": [
    "Next, we will check for any duplicate observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd0f7c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates\n",
    "print(f\"\\nNumber of duplicate rows: {df.duplicated().sum()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87fd4037",
   "metadata": {},
   "source": [
    "Luckily, we don't have to worry about duplicate rows. Now, let us check for rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b27747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# check for missing values\n",
    "print(f\"\\nNumber of missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edbe3bce",
   "metadata": {},
   "source": [
    "None of our observations have missing values. Therefore, we don't have to worry about holes in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aca8826",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = copy.deepcopy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1e36df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>Sex_int</th>\n",
       "      <th>ChestPainType_int</th>\n",
       "      <th>RestingECG_int</th>\n",
       "      <th>ExerciseAngina_int</th>\n",
       "      <th>ST_Slope_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.433140</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>0.410909</td>\n",
       "      <td>0.825070</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>1.382928</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.478484</td>\n",
       "      <td>F</td>\n",
       "      <td>NAP</td>\n",
       "      <td>1.491752</td>\n",
       "      <td>-0.171961</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.754157</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.751359</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>-0.129513</td>\n",
       "      <td>0.770188</td>\n",
       "      <td>0</td>\n",
       "      <td>ST</td>\n",
       "      <td>-1.525138</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.584556</td>\n",
       "      <td>F</td>\n",
       "      <td>ASY</td>\n",
       "      <td>0.302825</td>\n",
       "      <td>0.139040</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>-1.132156</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051881</td>\n",
       "      <td>M</td>\n",
       "      <td>NAP</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>-0.034755</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>-0.581981</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  \\\n",
       "0 -1.433140   M           ATA   0.410909     0.825070          0     Normal   \n",
       "1 -0.478484   F           NAP   1.491752    -0.171961          0     Normal   \n",
       "2 -1.751359   M           ATA  -0.129513     0.770188          0         ST   \n",
       "3 -0.584556   F           ASY   0.302825     0.139040          0     Normal   \n",
       "4  0.051881   M           NAP   0.951331    -0.034755          0     Normal   \n",
       "\n",
       "      MaxHR ExerciseAngina  Oldpeak ST_Slope  HeartDisease  Sex_int  \\\n",
       "0  1.382928              N      0.0       Up             0        1   \n",
       "1  0.754157              N      1.0     Flat             1        0   \n",
       "2 -1.525138              N      0.0       Up             0        1   \n",
       "3 -1.132156              Y      1.5     Flat             1        0   \n",
       "4 -0.581981              N      0.0       Up             0        1   \n",
       "\n",
       "   ChestPainType_int  RestingECG_int  ExerciseAngina_int  ST_Slope_int  \n",
       "0                  1               1                   0             2  \n",
       "1                  2               1                   0             1  \n",
       "2                  1               2                   0             2  \n",
       "3                  0               1                   1             1  \n",
       "4                  2               1                   0             2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# define vars to encode as integer    \n",
    "encoders = dict()\n",
    "categorical_headers = [\n",
    "    'Sex',\n",
    "    'ChestPainType',\n",
    "    'RestingECG',\n",
    "    'ExerciseAngina',\n",
    "    'ST_Slope'\n",
    "]\n",
    "\n",
    "# train all encoders\n",
    "for col in categorical_headers:\n",
    "    df_imputed[col] = df_imputed[col].str.strip()\n",
    "    \n",
    "    if col == 'HeartDisease':\n",
    "        tmp = LabelEncoder()\n",
    "        df_imputed[col] = tmp.fit_transform(df_imputed[col])\n",
    "    else:\n",
    "        # integer encode strings that are features\n",
    "        encoders[col] = LabelEncoder() # save the encoder\n",
    "        df_imputed[col+'_int'] = encoders[col].fit_transform(df_imputed[col])\n",
    "\n",
    "# scale numeric, continuous variables\n",
    "numeric_headers = [\n",
    "    \"Age\", \n",
    "    \"RestingBP\", \n",
    "    \"Cholesterol\",\n",
    "    \"MaxHR\"\n",
    "]\n",
    "\n",
    "ss = StandardScaler()\n",
    "df_imputed[numeric_headers] = ss.fit_transform(df_imputed[numeric_headers])\n",
    "\n",
    "include_header =[\"FastingBS\",\"Oldpeak\"]\n",
    "df_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74d0439e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use the following 11 features:\n",
      "[   'Sex_int',\n",
      "    'ChestPainType_int',\n",
      "    'RestingECG_int',\n",
      "    'ExerciseAngina_int',\n",
      "    'ST_Slope_int',\n",
      "    'Age',\n",
      "    'RestingBP',\n",
      "    'Cholesterol',\n",
      "    'MaxHR',\n",
      "    'FastingBS',\n",
      "    'Oldpeak']\n"
     ]
    }
   ],
   "source": [
    "# let's start without any feature preprocessing\n",
    "categorical_headers_ints = [x+'_int' for x in categorical_headers]\n",
    "feature_columns = categorical_headers_ints+numeric_headers+include_header\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "print(f\"We will use the following {len(feature_columns)} features:\")\n",
    "pp.pprint(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4e8164d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 918 entries, 0 to 917\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Age             918 non-null    int64  \n",
      " 1   Sex             918 non-null    object \n",
      " 2   ChestPainType   918 non-null    object \n",
      " 3   RestingBP       918 non-null    int64  \n",
      " 4   Cholesterol     918 non-null    int64  \n",
      " 5   FastingBS       918 non-null    int64  \n",
      " 6   RestingECG      918 non-null    object \n",
      " 7   MaxHR           918 non-null    int64  \n",
      " 8   ExerciseAngina  918 non-null    object \n",
      " 9   Oldpeak         918 non-null    float64\n",
      " 10  ST_Slope        918 non-null    object \n",
      " 11  HeartDisease    918 non-null    int64  \n",
      "dtypes: float64(1), int64(6), object(5)\n",
      "memory usage: 86.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3c6df2d",
   "metadata": {},
   "source": [
    "### Defining Class Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6696e73b",
   "metadata": {},
   "source": [
    "| Variable Name | Datatype | Description | Values |\n",
    ":------: | :------: | :------: | :------:|\n",
    "| `Age` | Numerical int | How old the patient is (yrs) | NA |\n",
    "| `Sex` | Categorical str | The biological gender of the patient | {**M**: Male, **F**: Female} |\n",
    "| `ChestPainType` | Categorical str | The chest pain condition of the patient | {**TA**: Typical Angina, **ATA**: Atypical Angina, **NAP**: Non-Anginal Pain, **ASY**: Asymptomatic} |\n",
    "| `RestingBP` | Numerical int | The resting blood pressure (mmHg) | NA |\n",
    "| `Cholesterol` | Numerical int | The cholesterol level of the patient (mm/dl) | NA |\n",
    "| `FastingBS` | Categorical int | The fasting blood sugar level of the patient | {**1**: if FastingBS > 120 (mg/dl), **0**: otherwise }|\n",
    "| `RestingECG` | Categorical str | The resting electrocardiogram results of the patient | {**Normal**: Normal, **ST**: Having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), **LVH**: Showing probable or definite left ventricular hypertrophy by Estes' criteria} |\n",
    "| `MaxHR` | Numerical int | The maximum heart rate recorded in the patient | NA |\n",
    "| `ExerciseAngina` | Categorical str | Whether the patient has exersice-induced angina | {**Y**: Yes, **N**: No} |\n",
    "| `OldPeak` | Numerical float | ST segment value of the patient | NA |\n",
    "| `ST_Slope` | Categorical str | Slope of the peak exercise ST segment | {**Up**: Upsloping, **Flat**: Flat, **Down**: Downsloping} |\n",
    "| `HeartDisease` | Categorical int | Whether the patient is likely to have heart failure| {**1**: Likely to have heart failure, **0**: Not likely to have heart failure }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b533efa4",
   "metadata": {},
   "source": [
    "Since all of our features are medically related to heart conditions and failure, we will not remove any features from our dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b52db25",
   "metadata": {},
   "source": [
    "### Cross Product Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46cd667a",
   "metadata": {},
   "source": [
    "Since our dataset has 12 features, including our target feature, and we don't want our performance to suffer due to overfitting, we will create 2 cross-product features.\n",
    "\n",
    "`Age` & `Sex`: Regarding heart failures and condition, older people are more likely to suffer heart failure. Looking only at gender, men are more at risk of having a heart faiure than women. The combination of these features would capture the effect that age and gender have on the likelihood of suffering heart failure. Therefore, we believe it is worth further exploring by combining these features into a cross product.\n",
    "\n",
    "`Cholesterol` & `RestingBP`: Cholesterol greartly affects the likelihood of someong having a heart failure. However, the team struggled to choose whether to combine it with `RestingBP` or `FastingBS`. Both features also affect the likelihood of heart failure. However, high blood pressure is a more consistant and reliable factor in patients who have heart failure. Additionally, high blood pressure is the leading cause of heart failure in the US. High blood sugar levels can also damage nerves and blood vessels, but it is less reliable since not all people with high blood sugar levels suffer heart failure in their lifetime. Therfore, we chose to create a cross product feature from `Cholesterol` and `RestingBP`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bf66f90",
   "metadata": {},
   "source": [
    "### Evaluation Method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf437a4d",
   "metadata": {},
   "source": [
    "### Dataset Splitting Method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b446802e",
   "metadata": {},
   "source": [
    "For this dataset, we will split it using the stratified 10-fold cross validation. Since we only have 918 observations, we need to maximize the use of our dataset. This method will also ensure that the subsets are as evenly distributed as possible, so that our model's performance does not skew towards one class or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29a53464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, ShuffleSplit, cross_val_score\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "X = df_imputed[feature_columns].to_numpy()\n",
    "y = df_imputed['HeartDisease'].values\n",
    "\n",
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "cv_object = StratifiedKFold(n_splits=num_cv_iterations)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3dcb0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "#lr_clf = HessianBinaryLogisticRegression(eta=0.1,iterations=10) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71e9ca33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 918\n",
      "n_features: 11\n",
      "n_classes: 2\n"
     ]
    }
   ],
   "source": [
    "# get some of the specifics of the dataset\n",
    "n_samples, n_features = X.shape\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "print(\"n_samples: {}\".format(n_samples))\n",
    "print(\"n_features: {}\".format(n_features))\n",
    "print(\"n_classes: {}\".format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66922b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in each class:[410 508]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "print('Number of instances in each class:'+str(np.bincount(y)))\n",
    "plt.hist(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc371e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.793921568627451, 0.9411764705882353)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# select model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X,y)\n",
    "\n",
    "#select cross validation\n",
    "cv = StratifiedShuffleSplit(n_splits=10) # made kfold stratified\n",
    "\n",
    "# select evaluation criteria\n",
    "my_scorer = make_scorer(recall_score)\n",
    "\n",
    "# run model training and cross validation\n",
    "per_fold_eval_criteria = cross_val_score(estimator=clf,\n",
    "                                    X=X,\n",
    "                                    y=y,\n",
    "                                    cv=cv,\n",
    "                                    scoring=my_scorer\n",
    "                                   )\n",
    "\n",
    "plt.bar(range(len(per_fold_eval_criteria)),per_fold_eval_criteria)\n",
    "plt.ylim([min(per_fold_eval_criteria)-0.01,max(per_fold_eval_criteria)])\n",
    "#print(per_fold_eval_criteria.mean()*100)\n",
    "\n",
    "\n",
    "# We have 508 values as 'HeartDisease' i.e. 1. From, above bar chart, we can see that few of the bars are 86% i.e. 1 denoting HeartDisease. So, some of folds have perfect recall. lowest here is 0.80, its 80% recall.\n",
    "# \n",
    "# We do not have severe class imbalance in our data set. We need to stratify accross all the folds that we use and make sure classes are stratified in each fold. In order to properly seperate training and testing sets a stratified KFold should be used. Stratified KFold will ensure that each fold is representative of the overall data set. This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.\n",
    "# \n",
    "# - Reference : [StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad13f01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use the following 11 features:\n",
      "[   'Sex_int',\n",
      "    'ChestPainType_int',\n",
      "    'RestingECG_int',\n",
      "    'ExerciseAngina_int',\n",
      "    'ST_Slope_int',\n",
      "    'Age',\n",
      "    'RestingBP',\n",
      "    'Cholesterol',\n",
      "    'MaxHR',\n",
      "    'FastingBS',\n",
      "    'Oldpeak']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "print(f\"We will use the following {len(feature_columns)} features:\")\n",
    "pp.pprint(feature_columns)\n",
    "\n",
    "\n",
    "# Identifying the groups of features in the data that should be combined into cross-product features:\n",
    "# \n",
    "# For this dataset, the categorical features are `Sex , ChestPainType , RestingECG, ExerciseAngina , ST_Slope`\n",
    "# \n",
    "# The cross-product operation on the categorical features can be interpreted as logical conjunctions. \n",
    "# The significance of the cross-product features is creating the combined features which are more useful \n",
    "# to the prediction/classification tasks compared with the independent features .\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b147127e",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c27d3ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.metrics import Recall\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21d04f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, lets setup the input size\n",
    "num_features = X_train.shape[1]\n",
    "input_tensor = Input(shape=(num_features,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(units=10, activation='relu')(input_tensor)\n",
    "x = Dense(units=5, activation='tanh')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(inputs=input_tensor, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73768b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 11)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                120       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181\n",
      "Trainable params: 181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/12\n",
      "26/26 [==============================] - 1s 2ms/step - loss: 0.3377 - recall: 0.0742\n",
      "Epoch 2/12\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.3259 - recall: 0.0939\n",
      "Epoch 3/12\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.3146 - recall: 0.1354\n",
      "Epoch 4/12\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.3039 - recall: 0.1572\n",
      "Epoch 5/12\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2939 - recall: 0.2162\n",
      "Epoch 6/12\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2847 - recall: 0.2620\n",
      "Epoch 7/12\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2762 - recall: 0.3341\n",
      "Epoch 8/12\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2684 - recall: 0.3930\n",
      "Epoch 9/12\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2614 - recall: 0.4716\n",
      "Epoch 10/12\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2549 - recall: 0.5459\n",
      "Epoch 11/12\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2488 - recall: 0.6114\n",
      "Epoch 12/12\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2432 - recall: 0.6528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19e36b06580>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[Recall()])\n",
    "\n",
    "model.summary()\n",
    "model.fit(X_train, y_train, epochs=12, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e33d3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot_ng as pydot\n",
    "import graphviz\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f849d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model( model, to_file='model.png', show_shapes=True, show_layer_names=True, rankdir='LR')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c18ac365",
   "metadata": {},
   "source": [
    "Now lets see how our model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fa07ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n",
      "[[10 31]\n",
      " [ 9 41]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.24      0.33        41\n",
      "           1       0.57      0.82      0.67        50\n",
      "\n",
      "    accuracy                           0.56        91\n",
      "   macro avg       0.55      0.53      0.50        91\n",
      "weighted avg       0.55      0.56      0.52        91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yhat_proba = model.predict(X_test)\n",
    "yhat = np.round(yhat_proba)\n",
    "print(mt.confusion_matrix(y_test,yhat))\n",
    "print(mt.classification_report(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6137f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 0s/step\n",
      "0.5604395604395604\n"
     ]
    }
   ],
   "source": [
    "yhat_proba = model.predict(X_test)\n",
    "yhat = np.round(yhat_proba)\n",
    "print(mt.accuracy_score(y_test,yhat))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af830d56",
   "metadata": {},
   "source": [
    "It is medical dataset, so it is necessary that we account for the true positives and false negatives. Predicting false negative values will be considered bad model. Hence, Recall should be used instead of accuracy due to the importance of false negatives for this data set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2ed93d0",
   "metadata": {},
   "source": [
    "### First Combination"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ec43236",
   "metadata": {},
   "source": [
    "In order to add one-hot encoding, we need to separate the categorical features that are currently saved as integers and place them into Embedding layers. An embedding layer deals with integers as if they were one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e522dd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " categorical (InputLayer)       [(None, 11)]         0           []                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather (TFOpLambd  (None,)             0           ['categorical[0][0]']            \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_1 (TFOpLam  (None,)             0           ['categorical[0][0]']            \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_2 (TFOpLam  (None,)             0           ['categorical[0][0]']            \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_3 (TFOpLam  (None,)             0           ['categorical[0][0]']            \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_4 (TFOpLam  (None,)             0           ['categorical[0][0]']            \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " Sex_int_embed (Embedding)      (None, 1)            2           ['tf.compat.v1.gather[0][0]']    \n",
      "                                                                                                  \n",
      " ChestPainType_int_embed (Embed  (None, 2)           8           ['tf.compat.v1.gather_1[0][0]']  \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " RestingECG_int_embed (Embeddin  (None, 1)           3           ['tf.compat.v1.gather_2[0][0]']  \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " ExerciseAngina_int_embed (Embe  (None, 1)           2           ['tf.compat.v1.gather_3[0][0]']  \n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " ST_Slope_int_embed (Embedding)  (None, 1)           3           ['tf.compat.v1.gather_4[0][0]']  \n",
      "                                                                                                  \n",
      " concat_1 (Concatenate)         (None, 6)            0           ['Sex_int_embed[0][0]',          \n",
      "                                                                  'ChestPainType_int_embed[0][0]',\n",
      "                                                                  'RestingECG_int_embed[0][0]',   \n",
      "                                                                  'ExerciseAngina_int_embed[0][0]'\n",
      "                                                                 , 'ST_Slope_int_embed[0][0]']    \n",
      "                                                                                                  \n",
      " combined (Dense)               (None, 1)            7           ['concat_1[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "# start by getting only the categorical variables\n",
    "# these matrices are all integers\n",
    "#X_train = df_train[categorical_headers_ints].to_numpy() \n",
    "#X_test = df_test[categorical_headers_ints].to_numpy() \n",
    "\n",
    "embed_branches = []\n",
    "all_branch_outputs = [] # this is where we will keep track of output of each branch\n",
    "\n",
    "# feed in the entire matrix of categircal variables\n",
    "input_branch = Input(shape=(X_train.shape[1],), \n",
    "                     dtype='int64', \n",
    "                     name='categorical')\n",
    "\n",
    "# for each categorical variable\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "    \n",
    "    # what the maximum integer value for this variable?\n",
    "    # which is the same as the number of categories\n",
    "    N = df_imputed[col].max()+1 \n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_branch, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs in list to concatenate later\n",
    "    all_branch_outputs.append(x)\n",
    "    \n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "final_branch = concatenate(all_branch_outputs, name='concat_1')\n",
    "final_branch = Dense(units=1,\n",
    "                     activation='sigmoid', \n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "model = Model(inputs=input_branch, outputs=final_branch)\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[Recall()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b942702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f840eeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "17/17 [==============================] - 1s 4ms/step - loss: 0.2533 - recall_1: 0.0000e+00\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.2504 - recall_1: 0.0546\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.2475 - recall_1: 0.3865\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2447 - recall_1: 0.7860\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2421 - recall_1: 0.8930\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.2396 - recall_1: 0.9039\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.2371 - recall_1: 0.9585\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.2347 - recall_1: 0.9607\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.2323 - recall_1: 0.9629\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.2301 - recall_1: 0.9629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19e38278760>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26eae22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 4ms/step\n",
      "[[26 15]\n",
      " [ 3 47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.63      0.74        41\n",
      "           1       0.76      0.94      0.84        50\n",
      "\n",
      "    accuracy                           0.80        91\n",
      "   macro avg       0.83      0.79      0.79        91\n",
      "weighted avg       0.82      0.80      0.80        91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yhat_proba = model.predict(X_test)\n",
    "yhat1 = np.round(yhat_proba)\n",
    "print(mt.confusion_matrix(y_test,yhat1))\n",
    "print(mt.classification_report(y_test,yhat1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a97a1a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5604395604395604\n"
     ]
    }
   ],
   "source": [
    "print(mt.accuracy_score(y_test,yhat))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28df765d",
   "metadata": {},
   "source": [
    "### Second Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba2942fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_X = df_imputed[categorical_headers_ints].to_numpy()\n",
    "# we want to predict the X and y data as follows:\n",
    "cat_y = df_imputed['HeartDisease'].values # get the labels we want\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(cat_X,cat_y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train_cat = cat_X[train_indices]\n",
    "    y_train_cat = cat_y[train_indices]\n",
    "    \n",
    "    X_test_cat = cat_X[test_indices]\n",
    "    y_test_cat = cat_y[test_indices]\n",
    "    \n",
    "num_X = df_imputed[categorical_headers_ints].to_numpy()\n",
    "# we want to predict the X and y data as follows:\n",
    "num_y = df_imputed['HeartDisease'].values # get the labels we want\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(num_X,num_y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train_num = num_X[train_indices]\n",
    "    y_train_num = num_y[train_indices]\n",
    "    \n",
    "    X_test_num = num_X[test_indices]\n",
    "    y_test_num = num_y[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92060593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " categorical (InputLayer)       [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_5 (TFOpLam  (None,)             0           ['categorical[0][0]']            \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_6 (TFOpLam  (None,)             0           ['categorical[0][0]']            \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_7 (TFOpLam  (None,)             0           ['categorical[0][0]']            \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_8 (TFOpLam  (None,)             0           ['categorical[0][0]']            \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_9 (TFOpLam  (None,)             0           ['categorical[0][0]']            \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " numeric (InputLayer)           [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " Sex_int_embed (Embedding)      (None, 1)            2           ['tf.compat.v1.gather_5[0][0]']  \n",
      "                                                                                                  \n",
      " ChestPainType_int_embed (Embed  (None, 2)           8           ['tf.compat.v1.gather_6[0][0]']  \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " RestingECG_int_embed (Embeddin  (None, 1)           3           ['tf.compat.v1.gather_7[0][0]']  \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " ExerciseAngina_int_embed (Embe  (None, 1)           2           ['tf.compat.v1.gather_8[0][0]']  \n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " ST_Slope_int_embed (Embedding)  (None, 1)           3           ['tf.compat.v1.gather_9[0][0]']  \n",
      "                                                                                                  \n",
      " num_1 (Dense)                  (None, 22)           132         ['numeric[0][0]']                \n",
      "                                                                                                  \n",
      " concat_1 (Concatenate)         (None, 28)           0           ['Sex_int_embed[0][0]',          \n",
      "                                                                  'ChestPainType_int_embed[0][0]',\n",
      "                                                                  'RestingECG_int_embed[0][0]',   \n",
      "                                                                  'ExerciseAngina_int_embed[0][0]'\n",
      "                                                                 , 'ST_Slope_int_embed[0][0]',    \n",
      "                                                                  'num_1[0][0]']                  \n",
      "                                                                                                  \n",
      " combined_1 (Dense)             (None, 10)           290         ['concat_1[0][0]']               \n",
      "                                                                                                  \n",
      " combined_2 (Dense)             (None, 1)            11          ['combined_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 451\n",
      "Trainable params: 451\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# save categorical features\n",
    "#X_train_cat = df_train[categorical_headers_ints].to_numpy() \n",
    "#X_test_cat = df_test[categorical_headers_ints].to_numpy() \n",
    "\n",
    "# and save off the numeric features\n",
    "#X_train_num =  df_[numeric_headers].to_numpy()\n",
    "#X_test_num = df_test[numeric_headers].to_numpy()\n",
    "\n",
    "all_branch_outputs = [] # this is where we will keep track of output of each branch\n",
    "\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "    \n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = df_imputed[col].max()+1\n",
    "    \n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    all_branch_outputs.append(x)\n",
    "    \n",
    "# HERE IS THE ADDING OF AN INPUT USING NUMERIC DATA\n",
    "# create dense input branch for numeric\n",
    "inputs_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=22, activation='relu', name='num_1')(inputs_num)\n",
    "    \n",
    "all_branch_outputs.append(x_dense)\n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "final_branch = concatenate(all_branch_outputs, name='concat_1')\n",
    "final_branch = Dense(units=10, activation='relu', name='combined_1')(final_branch)\n",
    "final_branch = Dense(units=1, activation='sigmoid', name='combined_2')(final_branch)\n",
    "\n",
    "model = Model(inputs=[input_cat,inputs_num], outputs=final_branch)\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[Recall()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cedd6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plot_model(model, \n",
    "           to_file='model.png',\n",
    "           show_shapes=True,\n",
    "           show_layer_names=True,\n",
    "           rankdir='LR',\n",
    "           expand_nested=False, \n",
    "           dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e60b5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "17/17 [==============================] - 1s 3ms/step - loss: 0.2563 - recall_3: 0.7969\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2542 - recall_3: 0.8166\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2521 - recall_3: 0.8930\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.2500 - recall_3: 0.8690\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2480 - recall_3: 0.8777\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2460 - recall_3: 0.9236\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2440 - recall_3: 0.9061\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.2422 - recall_3: 0.8886\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2403 - recall_3: 0.8865\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.2385 - recall_3: 0.8843\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "[[10 31]\n",
      " [ 9 41]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.24      0.33        41\n",
      "           1       0.57      0.82      0.67        50\n",
      "\n",
      "    accuracy                           0.56        91\n",
      "   macro avg       0.55      0.53      0.50        91\n",
      "weighted avg       0.55      0.56      0.52        91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=Recall())\n",
    "\n",
    "model.fit([ X_train_cat, X_train_num ], # inputs for each branch are a list\n",
    "          y_train, \n",
    "          epochs=10, \n",
    "          batch_size=50, \n",
    "          verbose=1)\n",
    "\n",
    "yhat2 = model.predict([X_test_cat,\n",
    "                      X_test_num]) # each branch has an input\n",
    "\n",
    "yhat2 = np.round(yhat)\n",
    "print(mt.confusion_matrix(y_test,yhat2))\n",
    "print(mt.classification_report(y_test,yhat2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caec8307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5604395604395604\n"
     ]
    }
   ],
   "source": [
    "print(mt.accuracy_score(y_test,yhat2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41a61f78",
   "metadata": {},
   "source": [
    "### Third Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f2dde17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChestPainType_RestingECG_ST_Slope', 'Sex_ExerciseAngina']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_columns = [['ChestPainType','RestingECG','ST_Slope'],\n",
    "                 #['sex', 'marital_status'],\n",
    "                 #['workclass','occupation'],\n",
    "                 #['occupation','race','education'],\n",
    "                 ['Sex','ExerciseAngina']\n",
    "                ]\n",
    "\n",
    "# cross each set of columns in the list above\n",
    "cross_col_df_names = []\n",
    "for cols_list in cross_columns:\n",
    "    # encode as ints for the embedding\n",
    "    enc = LabelEncoder()\n",
    "    \n",
    "    # 1. create crossed labels by join operation\n",
    "    X_crossed_train = df_imputed[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "   # X_crossed_test = df_test[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "    \n",
    "    # get a nice name for this new crossed column\n",
    "    cross_col_name = '_'.join(cols_list)\n",
    "    \n",
    "    # 2. encode as integers\n",
    "    enc.fit(np.hstack((X_crossed_train.to_numpy())))\n",
    "    \n",
    "    # 3. Save into dataframe with new name\n",
    "    df_imputed[cross_col_name] = enc.transform(X_crossed_train)\n",
    "    #df_test[cross_col_name] = enc.transform(X_crossed_test)\n",
    "    \n",
    "    # keep track of the new names of the crossed columns\n",
    "    cross_col_df_names.append(cross_col_name) \n",
    "    \n",
    "cross_col_df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de009bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_X = df_imputed[cross_col_df_names].to_numpy()\n",
    "# we want to predict the X and y data as follows:\n",
    "cross_y = df_imputed['HeartDisease'].values # get the labels we want\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(cross_X,cross_y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train_crossed = cross_X[train_indices]\n",
    "    y_train_crossed = cross_y[train_indices]\n",
    "    \n",
    "    X_test_crossed = cross_X[test_indices]\n",
    "    y_test_crossed = cross_y[test_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7214e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get crossed columns\n",
    "#X_train_crossed = df_train[cross_col_df_names].to_numpy()\n",
    "#X_test_crossed = df_test[cross_col_df_names].to_numpy()\n",
    "\n",
    "# save categorical features\n",
    "#X_train_cat = df_train[categorical_headers_ints].to_numpy() \n",
    "#X_test_cat = df_test[categorical_headers_ints].to_numpy() \n",
    "\n",
    "# and save off the numeric features\n",
    "#X_train_num =  df_train[numeric_headers].to_numpy()\n",
    "#X_test_num = df_test[numeric_headers].to_numpy()\n",
    "\n",
    "\n",
    "# we need to create separate lists for each branch\n",
    "crossed_outputs = []\n",
    "\n",
    "# CROSSED DATA INPUT\n",
    "input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='wide_inputs')\n",
    "for idx,col in enumerate(cross_col_df_names):\n",
    "    \n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = df_imputed[col].max()+1\n",
    "    \n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_crossed, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "    \n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = concatenate(crossed_outputs, name='wide_concat')\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# CATEGORICAL DATA INPUT\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical_input')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "    \n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = df_imputed[col].max()+1\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "    \n",
    "# NUMERIC DATA INPUT\n",
    "# create dense input branch for numeric\n",
    "input_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=22, activation='relu',name='num_1')(input_num)\n",
    "    \n",
    "all_deep_branch_outputs.append(x_dense)\n",
    "\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = concatenate(all_deep_branch_outputs,name='concat_embeds')\n",
    "deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "deep_branch = Dense(units=25,activation='relu', name='deep2')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "    \n",
    "# merge the deep and wide branch\n",
    "final_branch = concatenate([wide_branch, deep_branch],\n",
    "                           name='concat_deep_wide')\n",
    "final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "model = Model(inputs=[input_crossed,input_cat,input_num], \n",
    "              outputs=final_branch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa47db10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# you will need to install pydot properly on your machine to get this running\n",
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec6448fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['Recall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87771ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "26/26 [==============================] - 2s 16ms/step - loss: 0.2498 - recall: 0.0087 - val_loss: 0.2493 - val_recall: 0.0400\n",
      "Epoch 2/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.2471 - recall: 0.1769 - val_loss: 0.2479 - val_recall: 0.1800\n",
      "Epoch 3/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.2451 - recall: 0.2904 - val_loss: 0.2465 - val_recall: 0.2400\n",
      "Epoch 4/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.2432 - recall: 0.4148 - val_loss: 0.2452 - val_recall: 0.3000\n",
      "Epoch 5/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.2416 - recall: 0.4956 - val_loss: 0.2440 - val_recall: 0.3400\n",
      "Epoch 6/15\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2400 - recall: 0.5306 - val_loss: 0.2429 - val_recall: 0.3400\n",
      "Epoch 7/15\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2385 - recall: 0.5371 - val_loss: 0.2418 - val_recall: 0.3400\n",
      "Epoch 8/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.2370 - recall: 0.5852 - val_loss: 0.2408 - val_recall: 0.5200\n",
      "Epoch 9/15\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2356 - recall: 0.6266 - val_loss: 0.2397 - val_recall: 0.5400\n",
      "Epoch 10/15\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2342 - recall: 0.6463 - val_loss: 0.2387 - val_recall: 0.5600\n",
      "Epoch 11/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.2328 - recall: 0.6790 - val_loss: 0.2378 - val_recall: 0.6000\n",
      "Epoch 12/15\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2315 - recall: 0.6900 - val_loss: 0.2368 - val_recall: 0.6000\n",
      "Epoch 13/15\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2302 - recall: 0.7227 - val_loss: 0.2358 - val_recall: 0.6200\n",
      "Epoch 14/15\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2288 - recall: 0.7249 - val_loss: 0.2348 - val_recall: 0.6600\n",
      "Epoch 15/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.2275 - recall: 0.7489 - val_loss: 0.2338 - val_recall: 0.6600\n"
     ]
    }
   ],
   "source": [
    "# lets also add the history variable to see how we are doing\n",
    "# and lets add a validation set to keep track of our progress\n",
    "history = model.fit([X_train_crossed,X_train_cat,X_train_num],\n",
    "                    y_train, \n",
    "                    epochs=15, \n",
    "                    batch_size=32, \n",
    "                    verbose=1, \n",
    "                    validation_data = ([X_test_crossed,X_test_cat,X_test_num],y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afd09f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 4ms/step\n",
      "[[32  9]\n",
      " [17 33]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.78      0.71        41\n",
      "           1       0.79      0.66      0.72        50\n",
      "\n",
      "    accuracy                           0.71        91\n",
      "   macro avg       0.72      0.72      0.71        91\n",
      "weighted avg       0.73      0.71      0.71        91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yhat = np.round(model.predict([X_test_crossed,X_test_cat,X_test_num]))\n",
    "print(mt.confusion_matrix(y_test,yhat))\n",
    "print(mt.classification_report(y_test,yhat))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5bb8346",
   "metadata": {},
   "source": [
    "## Graduate Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
